# LLM Repository

HandsOn LLM applications using LangChain,Deep Lake,Cohere,Open API and other framewokrs. Langchain gives developers a set of tools and components to build LLM powered applications.


## Data Points

* The maximum number of tokens the model can process, is determined by the specific implementation of the LLM. Each model has token limit it can take in. Split the text into smaller chunks to handle large inputs.
* Few shot learning is the ability that allows LLMs to learn and genralize from limited examples. Examples can be both hard coded or dynamically selected and can be feeded to the LLM.
* occurrence of hallucinations - models can produce text that appears plausible on the surface but is actually factually incorrect or unrelated to the given input.
* LLMs may exhibit biases originating from their training data, resulting in outputs that can generate undesired outcomes.
* LLM examples include Text Summarization, Text Translation, and Question Answering
* Well-known models like GPT family, LLaMA employ subword level tokenization method. One of the variant used is Byte Pair Encoding (BPE)
* Character Text Splitter - This type of splitter can be used in various scenarios where you must split long text pieces into smaller, semantically meaningful chunks. For example, you might use it to split a long article into smaller chunks for easier processing or analysis
* The Recursive Character Text Splitter is a text splitter designed to split the text into chunks based on a list of characters provided.
* Embedding models are a type of machine learning model that convert discrete data into continuous vectors.





 